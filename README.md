# Cnservation of Causality: Generalizing Conservation of Information Makes the Physical Universe Possible, Renders Event Horizons Boring, and Unifies Physics

by Taylor Swift Fan

## Abstract
This paper proposes that conservation of information be generalized into the dimension of time so that it includes not only the conservation of instantaneous information, but also conservation of the integral of all extant information, including predictability and postdictability,for any given moment in time. The author is calling this conserved integral *causality*. Satisfaction of this new dimension of conservation ensures that the physical universe is able to exist, and enforces changes upon quantum mechanics, and perhaps general relativity, which the author suspects will allow the unification of physics.

## This Preprint's Limitations
The author of this paper is a retired electronics and software engineer, and their knowledge of general relativity and quantum mechanics is limited to fifty years of reading Scientific American and similar publications, which means that, in preprint form at least, this paper's standards of domain expertise and academic rigor will be unavoidably low. Please forward any instances of relevant priority to the author, who means well but just really doesn't know anything.

## Conservation of Causality
### The sum of the universe's causal information is constant
The current view of conservation of information says that information is never lost, and this has proven true in countless experiments.

The author proposes that information conservation stated in the conventional way is not fundamental, and is rather a special case of a more restrictive and more general law: the *conservation of causality*.

Establishing the meaning of causality requires starting with the most basic question about our physical universe, which is whether or not it is possible for it to have infinite longevity, yet contain finite information. A deterministic, non-repeating, spatially finite billiard-ball universe of infinite longevity would produce a predetermined history of infinite complexity, meaning that such a universe requires at least infinte precision for each property of each particle. The current model of quantum mechanics adds randomness, but it does so without reducing the information requirement below infinity. 

From this we see that if we can create a mathematical description of the universe which works with merely finite information and which contains no internal contradictions, then that description must be infinitely preferable to one which demands infinte information.

Conservation of causality enforces that constraint, which allows a spatially finite universe like ours to come into existence using only a finite amount of information.

Establishing that our universe must contain finite information despite its potentially infinite longevity immediately points to particles as the containers of very nearly all of the universe's information. Non-particle entities in the universe occur at very large scales, which makes their information-carrying capacity vanishingly small compared to that of particles.

Knowing that the universe's finite informaton is contained in particles tells us that the information contained in each particle is finite, which further tells us that the various parameters that compose the physical existence of the particle, such as position and momentum, must have limited precision. Of course it is assumed that the information is physically contained in the particle's mass-energy, since that is the simplest mechanism.

That limited precision is a (and perhaps the) primary source of fundamental randomness in our universe. Each time two particles interact, their interaction performs a calculation using the limited precision of the available information, and the calculation creates a result set of new particles whose total mass-energy and total information quantity are conserved, and that conserved information is encoded into the new particles. 

Although the new particles contain the same total amount of information as the old particles, they do *not* contain all of the information in the original particle set. Some of that information has been lost due to limited precision, and the new particles gain the complete certainty of their own state that comes from holding the defining data. This loss of precision represents a reduced ability to infer the state of the old particles from the information in the new particles. In other words, a small piece of the causal past has been lost forever, making the calculation only approximately reversible.

In our physical reality this loss is experimentally indectable over the smallish numbers of interactions that are captureed by experiment, giving the impression that information is conserved with infinite precision.

This loss accumulates exponentially with the number of interactions that take place, and after a sufficient number of interactions it reduces the causal significance of the more distant past to where it is lower in magnitude that the randomness injected into the process at each step by loss of precision, at which point the causal influence of any events farther in the past has reached exactly zero.

It seems possible that different particle interactions will lose differing amounts of causal information. For example, and interaction that only exchanges momentum would be expected to only lose momentum information, while an interaction that alters particle identity as well will be transforming more parameters, and therefore losing more information. This nuance is worth a lot more attention that I'm able to give it, and for now I'm going to make the simplifying assumption that these losses can be treated as statistically constant. 

If we look a toy system of isotopically stable gas held in thermal equilibrium, we can write a predictive equation.

* *P* is the average fraction of causal information retained in each interaction. This number is slightly less that one.
* *K* is the average number of interactions that each particle experiences per second.

 In equation form, causality is the integral of all extant information about how the universe interacts with itself from time negative infinity to time plus infinity for any given *t<sub>now</sub>*. 

 ![alt text](Causality.png)

## The meaning of *F<sub>sh</sub>*
The function *F<sub>sh</sub>* expresses how causality is discounted as time passes, which is far too complex to express in full for a complex system such as a rocky planet. But there are isolated portions of simple systems where it can be calculated, and a high-temperature and high pressure gas of stable isotopes at equilibrium is one such. 

As must be true for conservation laws, there is more than one way to derive this one. One way that the conservation of causality can be implied in full is by asserting that that a universe of finite spatial size but infinite longevity may be represented by a finite amount of information. If specifying a universe of finite size and infinite longevity using only a finite amount of information is physically possible, then our universe must contain only a finite amount of information. That's because parsimony insists that a universe which requires an infinite amount of information in order to exist must be infinitely less likely than a similar-looking universe requiring only a finite amount of information. 

Knowing that the information content of our universe is finite tells us that the informaiton content of each particle is also finite, and this tells us that the causal past for a particle containing finite information is also finite in time. This is because for the past to affect the present, those past effects must be encoded in the information that defines the particle somehow. If a past event has no representation in that particle's information, then it has no causal effect on whatever the particle might do next, so that past event is not part of the particle's causal past.

This <i>F</i>sh function captures the fact that shit happens, and it's hard not to notice that science laboratories have a hard time capturing this reality where it can be examined. Part of the issue of seing shit happen in the laboratory is that when shit does happen we stop calling it laboratory conditions. Your sample get zapped by a rare gargantuan cosmic ray shower? Throw that one out!


 is only nearly correct, and that error arises from the need to generalize this law to account for elapsed time.

Completing the full principle of conservation of information 

The fact that this value is constant over time also means that it cannot be infinite. This means that, contrary to the more common interpretation, specific bits of information are continually being created and destroyed, while only the universe's total quantity of information is conserved. Every subatomic particle interaction "creates" information in this view, and the math says that this must be causally connected, at least statistically, to an offsetting decrease in certainty in the rest of the sytsem. The value being conserved is the sum of all physically extant information about the present, plus all information about the past that can be inferred from the physically extant information about the present, plus all information about the future that can be predicted from physically extant information about the present. The bulk of physically extant information will cluster around the present, with inferrable information contributing less as time distance increases. Information is statistical in nature, which means that farther away an event is in time, the more the information must be discounted due to the exponentially cumulative degradation of statistical certainty over repeated temporal iterations.




This method of calculating information conservation makes it possible to define a finite universe of infinite longevity with a finite amount of information. It also implies that a deterministic non-repeating universe would contain an infinite amount of information at all times in its existence, which to the author seems appropriately impossible. 

This calculation of conserved information should have the effect of removing infinities from some calculations involving time because conserved causal information means causal time is finite; the ability to infer the past or predict the future degrades with increasing distance in time, and at some point it drops all the way to zero for smaller sizes and unstable structures. An obvious example is a gas held at constant high temperature and pressure. The precise state of such a gas has no causational correlation with states even a few seconds away, meaning that the past and future have reached their physical limits in finite time. Conversely, condensed matter at low temperatures has a potentially very lengthy but still finite future.

This calculation implies a special meaning for black holes, which is that rather than removing information from the universe, they merely remove information from the present and push it into the extremely distant future by evaporation where it is still part of the integral. Black holes are then remarkable for having a much longer causal future than other forms of matter. But the black hole's specific future information is not identical to the information that it absorbed in the past. Instead, the future information that is available is the object's emission spectrum predicted very precisely over a very long time. An emission spectrum is very little information by itself, but over the time required for the object to evaporate, that small amount of information becomes a very large amount.

The author lacks the skill to assess whether this conversion of lost past information to gained future predictability balances properly, and so cannot make any further informed comment, but the unique situation of evaporating black holes having such a strong effect on the total conserved information of the universe over very lengthy time periods does seem to suggest that the mathematical method for discounting information over various time distances might be closely related to the time evolution of evaporating black holes. 

Calculating conserved information in this new way places a limit on the kinds of universes that can make their way into being; universes that can come into being using only a finite amount of informaiton would have to be infinitely more common than universes which require an infinite amount of information to come into being.

They also make the possibly rash assumption that the math needed to integrate statistical information over time will arise fairly smoothly from standard statistical methods by enforcing this new conservation constraint, but those skills are also beyond them.

This proposal gives a more solid mathematical foundation to the fact that the future and past are more predictable and postdictable on larger scales than on smaller. It says that predictability and postdictability of the future and past reach zero at over even relatively small time distances at sub-atomic particle scales. 

Perhaps the simplest version of this new picture of the universe is that every subatomic particle is a multi-dimensional cloud of probability consisting of the time dimension, three spatial dimensions, plus the minimum number of dimensions required to describe its modes of interaction, and containing a constant amount of information. When one cloud interacts with another, the result is some new non-zero number of clouds whose total adds up to the original two. And the total system somehow conserves total information integrated over time. This suggests a physical limitation; that information is somehow encoded in the interactions between the particles of mass-energy composing physical reality. Parsimony suggests that the particles literally are the information, and vice-versa. The author presumes that some bright spark who, unlike the author, actually knows the relevant math will establish a one-to-one correspondence at some point.

Adding up the information in every multi-dimensional particle cloud in the universe at every instant (ignoring the speed of causality) gives a constant value, which means that every time two particles interact, something has happened, which increases the local knowledge in at least some dimensions. (The system went from not knowing something about what would happen next to knowing something about what just happened.) This means that somewhere in the system an offsetting amount of certainty about the past or future must be lost to compensate. It is appealing to presume that this constancy of information happens at the level of interaction, but it is perhaps possible that the conservation somehow happens at some other level.

To (yet again) say the same thing slightly differently, the past and future have finite limits that are a function of the scale and stability of the structures in the system, and beyond those limits the past has zero causal effect, meaning that the past stops at that point. Similarly, the future has finite limits to predictability, and beyond those limits the future is no longer physically meaningful because it exhibits zero causal connection to the present. If we are unable to affect a future by our actions, then it is not causally our future in any real sense.

The author suspects that any alternative to this viewpoint will require that proposed alternative universe to contain infinite information, making that alternate interpretation infinitely unlikely in comparison. They further suspect that integrating this version of the information conservation law into quantum mechanics and then into general relativity will not prove to be an intractable task, and that doing so will render the two mutually compatible. Of course, this is the cheerful optimism of someone who doesn't have to do the hard work.

The authorâ€™s mathematical skills break down completely here, and they hope that others will find this idea useful.
